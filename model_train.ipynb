{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFv35b6d93aAt1yJdaDnnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vichruth/Low-Parameter-ai-editor/blob/main/model_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hbBik6NubED"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q transformers==4.57.1 accelerate datasets torch torchvision sympy==1.13.1\n",
        "\n",
        "# STEP 2: Upload your dataset\n",
        "from google.colab import files\n",
        "print(\" Please upload your 'bug_fix_dataset.jsonl' file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "uploaded_filenames = list(uploaded.keys())\n",
        "if not uploaded_filenames:\n",
        "    raise ValueError(\" No file uploaded.\")\n",
        "data_file = uploaded_filenames[0]\n",
        "print(f\" Uploaded: {data_file}\")\n",
        "\n",
        "# STEP 3: Imports\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    RobertaTokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "def main():\n",
        "    # --- Load model ---\n",
        "    model_name = \"Salesforce/codet5-base\"\n",
        "    print(f\" Loading model: {model_name}\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # --- Load dataset ---\n",
        "    dataset = load_dataset(\"json\", data_files=data_file)\n",
        "    split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "    train_data, val_data = split[\"train\"], split[\"test\"]\n",
        "    print(f\" Train={len(train_data)}, Val={len(val_data)}\")\n",
        "\n",
        "    prefix = \"fix bug: \"\n",
        "\n",
        "    def preprocess_function(batch):\n",
        "        inputs = [prefix + code for code in batch[\"buggy_code\"]]\n",
        "        targets = batch[\"fixed_code\"]\n",
        "        model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    print(\" Tokenizing data...\")\n",
        "    train_dataset = train_data.map(preprocess_function, batched=True)\n",
        "    val_dataset = val_data.map(preprocess_function, batched=True)\n",
        "\n",
        "    collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "    # --- Training setup ---\n",
        "    use_fp16 = torch.cuda.is_available()\n",
        "    print(f\" GPU available: {use_fp16}\")\n",
        "    print(\"Transformers version:\", transformers.__version__)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./lowparam-bugfixer-model\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "        fp16=use_fp16,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "    print(\"\\n Starting fine-tuning …\")\n",
        "    trainer.train()\n",
        "    print(\"\\n Fine-tuning complete!\")\n",
        "\n",
        "    # --- Save model ---\n",
        "    final_path = \"./lowparam-bugfixer-model\"\n",
        "    trainer.save_model(final_path)\n",
        "    tokenizer.save_pretrained(final_path)\n",
        "    print(f\" Model saved to {final_path}\")\n",
        "\n",
        "    # --- Zip & download ---\n",
        "    print(\"Zipping model for download …\")\n",
        "    !zip -r lowparam-bugfixer-model.zip ./lowparam-bugfixer-model\n",
        "    try:\n",
        "        files.download(\"lowparam-bugfixer-model.zip\")\n",
        "        print(\"Download started.\")\n",
        "    except Exception as e:\n",
        "        print(f\" Auto-download failed: {e}\")\n",
        "        print(\"Please download manually from the left panel.\")\n",
        "\n",
        "# Run it all\n",
        "main()\n"
      ]
    }
  ]
}